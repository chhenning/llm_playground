{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1423b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0\n",
      "‚úÖ MPS (Apple Silicon GPU) is available!\n",
      "\n",
      "Loading model on MPS... (this may take a moment)\n",
      "\n",
      "Test Sentence: \"I am testing my Apple Silicon GPU and it feels blazing fast!\"\n",
      "Result: [{'label': 'POSITIVE', 'score': 0.9991268515586853}]\n",
      "Inference time: 0.0083 seconds\n",
      "\n",
      "Model is loaded on device: mps:0\n",
      "üéâ SUCCESS: Hugging Face is using your Apple GPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "def check_huggingface_mps():\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    # 1. Check if MPS (Apple Silicon GPU) is available\n",
    "    if not torch.backends.mps.is_available():\n",
    "        print(\"‚ùå MPS (Apple Silicon GPU) not detected.\")\n",
    "        print(\"Running on CPU instead.\")\n",
    "        device = \"cpu\"\n",
    "    else:\n",
    "        print(\"‚úÖ MPS (Apple Silicon GPU) is available!\")\n",
    "        device = \"mps\"\n",
    "\n",
    "    # 2. Load a simple model to test the pipeline\n",
    "    print(f\"\\nLoading model on {device.upper()}... (this may take a moment)\")\n",
    "    \n",
    "    try:\n",
    "        # We specify device=\"mps\" explicitly. \n",
    "        # Note: transformers pipeline handles device strings nicely in newer versions.\n",
    "        classifier = pipeline(\"sentiment-analysis\", device=device)\n",
    "        \n",
    "        # 3. Run a quick test\n",
    "        text = \"I am testing my Apple Silicon GPU and it feels blazing fast!\"\n",
    "        start_time = time.time()\n",
    "        result = classifier(text)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"\\nTest Sentence: \\\"{text}\\\"\")\n",
    "        print(f\"Result: {result}\")\n",
    "        print(f\"Inference time: {end_time - start_time:.4f} seconds\")\n",
    "        \n",
    "        # 4. Verify the model is actually on the GPU\n",
    "        model_device = next(classifier.model.parameters()).device\n",
    "        print(f\"\\nModel is loaded on device: {model_device}\")\n",
    "        \n",
    "        if model_device.type == 'mps':\n",
    "            print(\"üéâ SUCCESS: Hugging Face is using your Apple GPU.\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è NOTE: Model is running on CPU.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå An error occurred: {e}\")\n",
    "\n",
    "check_huggingface_mps()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
