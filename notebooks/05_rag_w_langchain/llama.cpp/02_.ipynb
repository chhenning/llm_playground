{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a105a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import requests\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a95a7a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_GLOB = \"data/pdfs/*.pdf\"\n",
    "PERSIST_DIR=\"temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed48db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_function():\n",
    "    \"\"\"\n",
    "    Returns an OpenAIEmbeddings instance configured for a local server.\n",
    "    \"\"\"\n",
    "    return OpenAIEmbeddings(\n",
    "        # 1. Point to your local server (exclude '/embeddings' from the URL here)\n",
    "        base_url=\"http://localhost:36912/v1\", \n",
    "        \n",
    "        # 2. Local servers usually ignore the key, but it cannot be empty\n",
    "        api_key=\"sk-no-key-required\", \n",
    "        \n",
    "        # 3. Some local servers require a model name (can often be anything)\n",
    "        model=\"Qwen3-Embedding-8B\",\n",
    "        \n",
    "        # 4. Disable validtion checks that might fail on local servers\n",
    "        check_embedding_ctx_length=False \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7588f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs(pdf_paths: List[str]):\n",
    "    \"\"\"Load PDFs into LangChain Documents (1 doc per page by default).\"\"\"\n",
    "    docs = []\n",
    "    for path in pdf_paths:\n",
    "        loader = PyPDFLoader(path)  # uses pypdf under the hood\n",
    "        docs.extend(loader.load())\n",
    "    return docs\n",
    "\n",
    "\n",
    "def build_vectorstore_from_pdfs(\n",
    "    pdf_glob: str,\n",
    "    persist_dir: str,\n",
    "    *,\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 200,\n",
    "):\n",
    "    pdf_paths = sorted(glob.glob(pdf_glob))\n",
    "    if not pdf_paths:\n",
    "        raise FileNotFoundError(f\"No PDFs matched: {pdf_glob}\")\n",
    "\n",
    "    raw_docs = load_pdfs(pdf_paths)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "    chunks = splitter.split_documents(raw_docs)\n",
    "\n",
    "    embeddings = get_embedding_function()\n",
    "    vs = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_dir,\n",
    "        collection_name=\"pdf_rag\",\n",
    "    )\n",
    "    return vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c0f4687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "vs = build_vectorstore_from_pdfs(PDF_GLOB, PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8b555e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb048092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Answer the question based only on the provided context.\n",
      "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "    Always return a 'SOURCES' section in your answer.\n",
      "    The 'SOURCES' section should contain the references from the source documents that you used to answer the question.\n",
      "\n",
      "    Context:\n",
      "    Interdum et malesuada fames ac ante ipsum primis in faucibus. Nunc a feugiat urna. Integer sit \n",
      "amet tincidunt mauris. Donec id urna nec turpis laoreet aliquam ac id ante. Curabitur faucibus \n",
      "consectetur velit, vitae efficitur odio facilisis sed. \n",
      " \n",
      "Aenean pulvinar euismod ligula at lacinia. Ut consectetur dui ipsum, a rhoncus lacus gravida \n",
      "vitae. In mollis tortor in libero lobortis molestie. Integer tempor justo ligula, eu euismod lectus \n",
      "fringilla eu. Proin vestibulum sodales tr istique. Pellentesque pretium, nibh et aliquet \n",
      "scelerisque, felis nulla lobortis tellus, at tristique libero ipsum a leo. Nulla mauris turpis, feugiat \n",
      "eu lacus eu, eleifend malesuada lorem. Praesent quis justo ligula. Cras quam risus, ultricies at \n",
      "odio acc umsan, maximus eleifend justo. Sed sed convallis elit. In finibus congue mauris at \n",
      "venenatis. Praesent pellentesque lacus eros, nec auctor neque semper eget. Sed vehicula\n",
      "\n",
      "Interdum et malesuada fames ac ante ipsum primis in faucibus. Nunc a feugiat urna. Integer sit \n",
      "amet tincidunt mauris. Donec id urna nec turpis laoreet aliquam ac id ante. Curabitur faucibus \n",
      "consectetur velit, vitae efficitur odio facilisis sed. \n",
      " \n",
      "Aenean pulvinar euismod ligula at lacinia. Ut consectetur dui ipsum, a rhoncus lacus gravida \n",
      "vitae. In mollis tortor in libero lobortis molestie. Integer tempor justo ligula, eu euismod lectus \n",
      "fringilla eu. Proin vestibulum sodales tr istique. Pellentesque pretium, nibh et aliquet \n",
      "scelerisque, felis nulla lobortis tellus, at tristique libero ipsum a leo. Nulla mauris turpis, feugiat \n",
      "eu lacus eu, eleifend malesuada lorem. Praesent quis justo ligula. Cras quam risus, ultricies at \n",
      "odio acc umsan, maximus eleifend justo. Sed sed convallis elit. In finibus congue mauris at \n",
      "venenatis. Praesent pellentesque lacus eros, nec auctor neque semper eget. Sed vehicula\n",
      "\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec iaculis elit nec ante eleifend, \n",
      "eget cursus tortor auctor. Integer at ultrices lorem, eget bibendum turpis. Praesent lacus \n",
      "sapien, ullamcorper gravida suscipit eu , pharetra et eros. Integer interdum dictum volutpat. \n",
      "Nulla pellentesque ante vitae sapien luctus ornare. Ut a risus quis diam rhoncus rhoncus. \n",
      "Integer tortor lorem, vulputate non tempus eu, suscipit id tellus. In laoreet sollicitudin quam, et \n",
      "condimentum justo ultrices eget. Pellentesque hendrerit suscipit diam ac porttitor. Aenean \n",
      "dictum id magna vel venenatis. In rhoncus tempus libero, sed consectetur nibh ultrices nec. \n",
      "Aenean pulvinar euismod ligula at lacinia. Ut consectetur dui ipsum, a rhoncus lacus gravida \n",
      "vitae. In mollis tortor in libero lobortis molestie. Integer tempor justo ligula, eu euismod lectus \n",
      "fringilla eu. Proin vestibulum sodales tristique. Pellentesque pretium, nibh et aliquet\n",
      "\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec iaculis elit nec ante eleifend, \n",
      "eget cursus tortor auctor. Integer at ultrices lorem, eget bibendum turpis. Praesent lacus \n",
      "sapien, ullamcorper gravida suscipit eu , pharetra et eros. Integer interdum dictum volutpat. \n",
      "Nulla pellentesque ante vitae sapien luctus ornare. Ut a risus quis diam rhoncus rhoncus. \n",
      "Integer tortor lorem, vulputate non tempus eu, suscipit id tellus. In laoreet sollicitudin quam, et \n",
      "condimentum justo ultrices eget. Pellentesque hendrerit suscipit diam ac porttitor. Aenean \n",
      "dictum id magna vel venenatis. In rhoncus tempus libero, sed consectetur nibh ultrices nec. \n",
      "Aenean pulvinar euismod ligula at lacinia. Ut consectetur dui ipsum, a rhoncus lacus gravida \n",
      "vitae. In mollis tortor in libero lobortis molestie. Integer tempor justo ligula, eu euismod lectus \n",
      "fringilla eu. Proin vestibulum sodales tristique. Pellentesque pretium, nibh et aliquet\n",
      "\n",
      "    Question:\n",
      "    Ut  consectetur  dui  ipsum,  a  rhoncus  lacus  gravida vitae. In mollis tortor in libero lobortis molestie?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "QUESTION = \"Ut  consectetur  dui  ipsum,  a  rhoncus  lacus  gravida vitae. In mollis tortor in libero lobortis molestie?\"\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"Helper to join document content into a single string.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def load_vectorstore(persist_dir: str):\n",
    "    embeddings = get_embedding_function()\n",
    "    return Chroma(\n",
    "        persist_directory=persist_dir,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"pdf_rag\",\n",
    "    )\n",
    "\n",
    "def chat(prompt_value):\n",
    "    \"\"\"\n",
    "    This function acts as the LLM.\n",
    "    It receives a LangChain 'PromptValue' and must return a string.\n",
    "    \"\"\"    \n",
    "    # Convert the prompt object (containing system/human messages) to a single string\n",
    "    prompt_text = prompt_value.to_string()\n",
    "\n",
    "    print(prompt_text)\n",
    "\n",
    "    try:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_text,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        data = {\"messages\": messages, \"temperature\": 0.7}\n",
    "        response = requests.post(\n",
    "            url=\"http://localhost:32001/v1/chat/completions\",\n",
    "            headers={\n",
    "                \"Content-Type\": \"application/json\",\n",
    "            },\n",
    "            data=json.dumps(data),\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "\n",
    "        return response.json()['choices'][0]['message']['content']\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "def make_rag_chain(vectorstore: Chroma):\n",
    "    # Create a prompt template using langchain_core.prompts\n",
    "    template = \"\"\"Answer the question based only on the provided context.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Always return a 'SOURCES' section in your answer.\n",
    "    The 'SOURCES' section should contain the references from the source documents that you used to answer the question.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {input}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    llm = RunnableLambda(chat)\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "    # Build the RAG chain using RunnablePassthrough and StrOutputParser\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever | format_docs,  # Ensure context is string for the prompt\n",
    "            \"input\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "vs = load_vectorstore(PERSIST_DIR)\n",
    "\n",
    "\n",
    "chain = make_rag_chain(vs)\n",
    "result = chain.invoke(QUESTION)  # Pass the question directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d381ccaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The text provided contains the sentence: \"Ut consectetur dui ipsum, a rhoncus lacus gravida vitae. In mollis tortor in libero lobortis molestie.\" This appears to be a fragment from a larger document, possibly a style guide or a sample of placeholder text (like Lorem Ipsum), but it does not provide enough context to answer any specific question about its meaning or purpose.\\n\\nTherefore, based on the information given, I cannot provide a meaningful answer to the question.\\n\\nSOURCES\\n- The provided context text, which includes the sentence in question.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
